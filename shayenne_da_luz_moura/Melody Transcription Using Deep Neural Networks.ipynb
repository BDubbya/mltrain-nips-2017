{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>SINGING VOICE TRANSCRIPTION USING DEEP NEURAL NETWORKS</h1></center> \n",
    "\n",
    "<h1><center>Implementation</center></h1>\n",
    "\n",
    "CODE AUTHOR: Shayenne Moura, Computer Music Group, University of São Paulo/Brazil\n",
    "\n",
    "*Original article by François Rigaud and Mathieu Radenen, ISMIR 2016.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This poster presents an implementation of a system for singing voice melody transcription based on an article published in ISMIR 2016. This system is not the original one from the paper, since details of implementation are not available. The main goal was to evaluate the reproducibility of articles published without source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The main task of this system is to estimate the main melody, i.e., sequences of fundamental frequency ($f_0$) where voice or melodic instrument are active.\n",
    "\n",
    "The realized approach was based in a supervised classification problem. It means that the system need to assign a pitch class for each frame if melody is present, otherwise, unvoiced class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System description\n",
    "\n",
    "I implemented a system proposed as illustrated in figure below. Two Deep Neural Networks (DNNs) are composed in parallel for identify melodic sequences in polyphonic music.\n",
    "\n",
    "<img src=\"img/systemDNN.png\" alt=\"System Overview\" style=\"width: 300px;\"/>\n",
    "\n",
    "The Voice Activity Detection (VAD) DNN is responsible for deciding which frames are voiced or unvoiced.\n",
    "\n",
    "The F0 estimation DNN is responsible for classifying a frequency class for each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess audio signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages\n",
      "Requirement already satisfied: joblib>=0.7.0 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from librosa)\n",
      "Requirement already satisfied: numpy>=1.8.0 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from librosa)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from librosa)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from librosa)\n",
      "Requirement already satisfied: resampy>=0.1.2 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from librosa)\n",
      "Requirement already satisfied: scikit-learn>=0.14.0 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from librosa)\n",
      "Requirement already satisfied: scipy>=0.13.0 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from librosa)\n",
      "Requirement already satisfied: six>=1.3 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from librosa)\n",
      "Requirement already satisfied: numba>=0.32 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from resampy>=0.1.2->librosa)\n",
      "Requirement already satisfied: llvmlite in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from numba>=0.32->resampy>=0.1.2->librosa)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: llvmlite in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade llvmlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules    \n",
    "from librosa import *            # Manipulate and display audio files\n",
    "import librosa.display\n",
    "import scipy                     # Utilize signal functions\n",
    "import matplotlib.pyplot as plt  # Plot graphics\n",
    "%matplotlib inline\n",
    "import numpy as np               # Manipulate arrays efficiently\n",
    "import os, glob                  # Find files in directories\n",
    "import IPython                   # Display audio file in notebook\n",
    "import csv                       # Manipulate .csv files\n",
    "import pandas as pd               # Manipulate data as matrix\n",
    "\n",
    "\n",
    "# Keras modules\n",
    "from keras.models import Model, Sequential, model_from_json, load_model\n",
    "\n",
    "# Put your audio file path \n",
    "your_path = \"./\"\n",
    "file_path = your_path + \"MusicDelta_Rockabilly_MIX.wav\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps to pre process the input data\n",
    "\n",
    "1. Converted to mono\n",
    "1. Re-sampled to 16kHz\n",
    "1. Double-stage Harmonic/Percussive Source Separation\n",
    "    - $h_1$ and $p_1$ (FFT using window of 256ms) by re-sampled signal\n",
    "    - $h_2$ and $p_2$ (FFT using window of 256ms) by $p_1$ signal reconstructed\n",
    "    \n",
    "    \n",
    "*Intuition:* This double-stage Harmonic/Percussive Source Separation enhances the separation between more stationary/stable sounds (the harmonic background) and more percussive instruments (such as drum). Melodic signals are usually present on $h_2$ spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-12-07 20:44:07--  https://www.ime.usp.br/~shayenne/nips/25SVD2model.h5\n",
      "Resolving webproxy (webproxy)... 192.168.1.101\n",
      "Connecting to webproxy (webproxy)|192.168.1.101|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 700800 (684K)\n",
      "Saving to: '25SVD2model.h5'\n",
      "\n",
      "25SVD2model.h5      100%[===================>] 684.38K  1003KB/s    in 0.7s    \n",
      "\n",
      "2017-12-07 20:44:08 (1003 KB/s) - '25SVD2model.h5' saved [700800/700800]\n",
      "\n",
      "--2017-12-07 20:44:09--  https://www.ime.usp.br/~shayenne/nips/25SVD2model.json\n",
      "Resolving webproxy (webproxy)... 192.168.1.101\n",
      "Connecting to webproxy (webproxy)|192.168.1.101|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 3814 (3.7K) [application/json]\n",
      "Saving to: '25SVD2model.json'\n",
      "\n",
      "25SVD2model.json    100%[===================>]   3.72K  --.-KB/s    in 0s      \n",
      "\n",
      "2017-12-07 20:44:09 (105 MB/s) - '25SVD2model.json' saved [3814/3814]\n",
      "\n",
      "--2017-12-07 20:44:10--  https://www.ime.usp.br/~shayenne/nips/MusicDelta_Rockabilly_MIX.wav\n",
      "Resolving webproxy (webproxy)... 192.168.1.101\n",
      "Connecting to webproxy (webproxy)|192.168.1.101|:3128... connected.\n",
      "Proxy request sent, awaiting response... 403 Forbidden\n",
      "2017-12-07 20:44:10 ERROR 403: Forbidden.\n",
      "\n",
      "--2017-12-07 20:44:11--  https://www.ime.usp.br/~shayenne/nips/MusicDelta_Rockabilly_MIX_features.csv\n",
      "Resolving webproxy (webproxy)... 192.168.1.101\n",
      "Connecting to webproxy (webproxy)|192.168.1.101|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 7023888 (6.7M) [text/csv]\n",
      "Saving to: 'MusicDelta_Rockabilly_MIX_features.csv'\n",
      "\n",
      "MusicDelta_Rockabil 100%[===================>]   6.70M  5.41MB/s    in 1.2s    \n",
      "\n",
      "2017-12-07 20:44:13 (5.41 MB/s) - 'MusicDelta_Rockabilly_MIX_features.csv' saved [7023888/7023888]\n",
      "\n",
      "--2017-12-07 20:44:13--  https://www.ime.usp.br/~shayenne/nips/MusicDelta_Rockabilly_MIX_mfcc.csv\n",
      "Resolving webproxy (webproxy)... 192.168.1.101\n",
      "Connecting to webproxy (webproxy)|192.168.1.101|:3128... connected.\n",
      "Proxy tunneling failed: Service UnavailableUnable to establish SSL connection.\n",
      "--2017-12-07 20:45:13--  https://www.ime.usp.br/~shayenne/nips/MusicDelta_Rockabilly_labels.csv\n",
      "Resolving webproxy (webproxy)... 192.168.1.101\n",
      "Connecting to webproxy (webproxy)|192.168.1.101|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 12246 (12K) [text/csv]\n",
      "Saving to: 'MusicDelta_Rockabilly_labels.csv'\n",
      "\n",
      "MusicDelta_Rockabil 100%[===================>]  11.96K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2017-12-07 20:45:14 (12.6 MB/s) - 'MusicDelta_Rockabilly_labels.csv' saved [12246/12246]\n",
      "\n",
      "--2017-12-07 20:45:14--  https://www.ime.usp.br/~shayenne/nips/f0modelNEW.h5\n",
      "Resolving webproxy (webproxy)... 192.168.1.101\n",
      "Connecting to webproxy (webproxy)|192.168.1.101|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 2431732 (2.3M)\n",
      "Saving to: 'f0modelNEW.h5'\n",
      "\n",
      "f0modelNEW.h5       100%[===================>]   2.32M  2.46MB/s    in 0.9s    \n",
      "\n",
      "2017-12-07 20:45:16 (2.46 MB/s) - 'f0modelNEW.h5' saved [2431732/2431732]\n",
      "\n",
      "--2017-12-07 20:45:16--  https://www.ime.usp.br/~shayenne/nips/f0modelNEW.json\n",
      "Resolving webproxy (webproxy)... 192.168.1.101\n",
      "Connecting to webproxy (webproxy)|192.168.1.101|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 1555 (1.5K) [application/json]\n",
      "Saving to: 'f0modelNEW.json'\n",
      "\n",
      "f0modelNEW.json     100%[===================>]   1.52K  --.-KB/s    in 0s      \n",
      "\n",
      "2017-12-07 20:45:17 (232 MB/s) - 'f0modelNEW.json' saved [1555/1555]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you need to download the following files\n",
    "!wget https://www.ime.usp.br/~shayenne/nips/25SVD2model.h5\n",
    "!wget https://www.ime.usp.br/~shayenne/nips/25SVD2model.json\n",
    "!wget https://www.ime.usp.br/~shayenne/nips/MusicDelta_Rockabilly_MIX.wav\n",
    "!wget https://www.ime.usp.br/~shayenne/nips/MusicDelta_Rockabilly_MIX_features.csv\n",
    "!wget https://www.ime.usp.br/~shayenne/nips/MusicDelta_Rockabilly_MIX_mfcc.csv\n",
    "!wget https://www.ime.usp.br/~shayenne/nips/MusicDelta_Rockabilly_labels.csv\n",
    "!wget https://www.ime.usp.br/~shayenne/nips/f0modelNEW.h5\n",
    "!wget https://www.ime.usp.br/~shayenne/nips/f0modelNEW.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessing...\n",
      "./MusicDelta_Rockabilly_MIX.wav\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/nbuser/library/shayenne_da_luz_moura/MusicDelta_Rockabilly_MIX.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6d5804705e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Preprocess audio signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Converted to mono\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_16k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_420/lib/python3.5/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0maudioread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0msr_native\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mn_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_420/lib/python3.5/site-packages/audioread/__init__.py\u001b[0m in \u001b[0;36maudio_open\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrawread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrawread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRawAudioFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_420/lib/python3.5/site-packages/audioread/rawread.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/nbuser/library/shayenne_da_luz_moura/MusicDelta_Rockabilly_MIX.wav'"
     ]
    }
   ],
   "source": [
    "print (\"--- Preprocessing...\")\n",
    "print (file_path)\n",
    "    \n",
    "# Preprocess audio signal\n",
    "y, sr = librosa.load(file_path, mono=True) # Converted to mono\n",
    "y_16k = librosa.resample(y, sr, 16000)\n",
    "sr = 16000\n",
    "\n",
    "print (\"> Audio signal loaded...\")\n",
    "\n",
    "# Applying HPSS separation\n",
    "# High frequency resolution - more clearly the frequencies\n",
    "print (\"> First HPSS decomposition (high-frequency resolution)...\")\n",
    "\n",
    "# STFT with Hamming window of 256ms (4096 samples) with overlap 0,75 (hop 0.25)\n",
    "s = librosa.stft(y_16k, n_fft=4096, hop_length=int(4096/4), window=scipy.signal.hamming(4096))\n",
    "h1, p1 = librosa.decompose.hpss(s)\n",
    "    \n",
    "    \n",
    "# P1 here has other frequency resolution\n",
    "print (\"> Second HPSS decomposition (high-frequency resolution)...\")\n",
    "# Getting signal - need arguments of stft\n",
    "p1_inverse = librosa.istft(p1, hop_length=int(4096/4), window=scipy.signal.hamming(4096))\n",
    "h1_inverse = librosa.istft(h1, hop_length=int(4096/4), window=scipy.signal.hamming(4096))\n",
    "\n",
    "\n",
    "# STFT with Hamming window of 32ms (512 samples) with overlap 0,75 (hop 0.25)\n",
    "p1_32 = librosa.stft(p1_inverse, n_fft=512, hop_length=int(512/4), window=scipy.signal.hamming(512))\n",
    "h2, p2 = librosa.decompose.hpss(p1_32)   \n",
    "\n",
    "# Getting signal separated - need arguments of stft\n",
    "h2_inverse = librosa.istft(h2, hop_length=int(512/4), window=scipy.signal.hamming(512))\n",
    "p2_inverse = librosa.istft(p2, hop_length=int(512/4), window=scipy.signal.hamming(512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps to pre process input for VAD\n",
    "\n",
    "For each of the reconstructed signals $h_1$, $h_2$ and $p_2$, timbral features are computed:\n",
    "   - MFCCs with 32ms Hamming windows, 0.75 of overlap, 40 triangular filters on mel scale between 0 and 8kHz.\n",
    "   - MFCCs, as describe above, normalized using mean and variance over its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h1_inverse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3b39d42a02f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" Input for VAD \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# The input for VAD is the whole signal s = h1 + h2 + p2 in MFCC features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mh1_mfcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh1_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mh2_mfcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh2_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mp2_mfcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp2_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h1_inverse' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" Input for VAD \"\"\"    \n",
    "# The input for VAD is the whole signal s = h1 + h2 + p2 in MFCC features\n",
    "h1_mfcc = librosa.feature.melspectrogram(y=h1_inverse, sr=sr, n_fft=512, hop_length=128, n_mels=40, fmax=8000)\n",
    "h2_mfcc = librosa.feature.melspectrogram(y=h2_inverse, sr=sr, n_fft=512, hop_length=128, n_mels=40, fmax=8000)\n",
    "p2_mfcc = librosa.feature.melspectrogram(y=p2_inverse, sr=sr, n_fft=512, hop_length=128, n_mels=40, fmax=8000)\n",
    "\n",
    "## Rescale done for VAD (Better results)\n",
    "h1_mfcc = h1_mfcc - h1_mfcc.mean()\n",
    "h1_mfcc = h1_mfcc / h1_mfcc.std()  \n",
    "\n",
    "h2_mfcc = h2_mfcc - h2_mfcc.mean()\n",
    "h2_mfcc = h2_mfcc / h2_mfcc.std()  \n",
    "\n",
    "p2_mfcc = p2_mfcc - p2_mfcc.mean()\n",
    "p2_mfcc = p2_mfcc / p2_mfcc.std()  \n",
    "    \n",
    "# Write mfcc file\n",
    "with open(file_path[:-4]+'_mfcc.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    col = h1_mfcc.shape[1]\n",
    "    row = np.ndarray(shape=(3*40,))\n",
    "    for i in range(col):\n",
    "        row[  :40] = h1_mfcc[:,i]\n",
    "        row[40:80] = h2_mfcc[:,i]\n",
    "        row[80:  ] = p2_mfcc[:,i]\n",
    "        spamwriter.writerow(row) ### Put the mfcc from h1+h2+p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps to pre process input for F0-estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each signal $p_1$, the features selected are:\n",
    "   - Log-spectrograms with 64ms Hamming windows and 0.75 of overlap [without discarting]\n",
    "   - Rescaled between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Input for F0-Detection \"\"\"    \n",
    "# Getting signal - need arguments of stft - Not discarting frequencies above 4kH\n",
    "p1_toF0_inverse = librosa.istft(p1, hop_length=int(4096/4), window=scipy.signal.hamming(4096))\n",
    "\n",
    "# STFT with Hamming window of 64ms (1024 samples) with overlap 0,75 (hop 0.25)\n",
    "p1_toF0 = librosa.stft(p1_toF0_inverse, n_fft=1024, hop_length=int(1024/4), window=scipy.signal.hamming(1024))\n",
    "# Log-spectrogram\n",
    "p1_toF0_log = librosa.amplitude_to_db(p1_toF0, ref=np.max)\n",
    "    \n",
    "## Rescale done for F0 Detection Model (Not cutted)\n",
    "p1_toF0_log = p1_toF0_log - p1_toF0_log.min()\n",
    "p1_toF0_log = p1_toF0_log / p1_toF0_log.max()  \n",
    "    \n",
    "\n",
    "# Write features file \n",
    "with open(file_path[:-4]+'_features.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    col = p1_toF0_log.shape[1]\n",
    "    for i in range(col):\n",
    "        spamwriter.writerow(p1_toF0_log[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">** IMPORTANT **</span>\n",
    "\n",
    "The training step is presented here only for visualization. The real training was made separatelly, using the Python script on the repository. The code presented below is a transcription from the one really used for training the deep neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### VAD Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Read all files in path and concatenate them \n",
    "print(\"Loading features for training...\")\n",
    "path =r'/var/tmp/IA/mfcc' # use your path\n",
    "allFiles = glob.glob(path + \"/*mfcc.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    print(\"   Load\", file_)\n",
    "    df = pd.read_csv(file_,index_col=None, header=None)\n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_)\n",
    "\n",
    "# Read all files in path and concatenate them \n",
    "print(\"Loading labels for training...\")\n",
    "path =r'/var/tmp/IA/labels' # use your path\n",
    "allFiles = glob.glob(path + \"/*labels.csv\")\n",
    "label = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=None)\n",
    "    list_.append(df)\n",
    "label = pd.concat(list_)\n",
    "\n",
    "\n",
    "df2=pd.DataFrame.as_matrix(frame)\n",
    "print (df2.shape)\n",
    "# Trying to rescale the data\n",
    "df2 = df2 - df2.min()\n",
    "df2 = df2 / df2.max()\n",
    "\n",
    "\n",
    "df4=pd.DataFrame.as_matrix(label)\n",
    "df4[df4 > 0] = 1\n",
    "print (df4.shape)\n",
    "\n",
    "print(\"Data's shape:\", df2.shape)\n",
    "\n",
    "# create data\n",
    "x_train = df2\n",
    "y_train = df4\n",
    "\n",
    "# Important variables\n",
    "max_features = 120\n",
    "maxlen = 120\n",
    "batch_size = 10\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "\n",
    "# create model\n",
    "print(\"Create model...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 50, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(50)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "print(\"Compile model...\")\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "print(\"Starting model training...\")\n",
    "for i in range(10):\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=1,\n",
    "              validation_split=0.1, shuffle=True)\n",
    "    print(\"> Finished\", i , \"epochs\")\n",
    "        # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"25SVD2model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"25SVD2model.h5\")\n",
    "    print(\"> Saved model to disk\")\n",
    "          \n",
    "# evaluate the model          \n",
    "scores = model.evaluate(x_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F0-estimation Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Read all files in path and concatenate them \n",
    "print(\"Loading features and labels for training...\")\n",
    "path_features =r'/var/tmp/IA/features/' # use your path\n",
    "path_label =r'/var/tmp/IA/labels/' # use your path\n",
    "\n",
    "list_features = []\n",
    "list_labels = []\n",
    "\n",
    "# Loading files\n",
    "for filename in os.listdir(path_features):\n",
    "    \n",
    "    if filename.endswith(\"features.csv\"):\n",
    "        music = filename\n",
    "        music = music[:-12]\n",
    "        print (music)\n",
    "        print(\"   Load\", path_features+music+\"features.csv\")\n",
    "        d1 = pd.read_csv(path_features+music+\"features.csv\",index_col=None, header=None)\n",
    "        list_features.append(d1)\n",
    "        print(\"   Load\", path_label+music+\"labels.csv\")\n",
    "        d2 = pd.read_csv(path_label+music+\"labels.csv\",index_col=None, header=None)\n",
    "        list_labels.append(d2)\n",
    "        \n",
    "# Grouping data \n",
    "frame = pd.concat(list_features)\n",
    "label = pd.concat(list_labels)\n",
    "\n",
    "\n",
    "# Formating data\n",
    "df2=pd.DataFrame.as_matrix(frame)\n",
    "\n",
    "df4=pd.DataFrame.as_matrix(label)\n",
    "df4 = df4[::2] # Hop size is doubled in features\n",
    "\n",
    "df5 = np.zeros(shape=(df2.shape[0],1))\n",
    "df5[:df4.shape[0],:] = df4\n",
    "\n",
    "print(\"Data's shape:\", df2.shape)\n",
    "print(\"Data's shape:\", df5.shape)\n",
    "\n",
    "\n",
    "# Remove no vocal frames - train with only vocal frames\n",
    "print(\"Removing no vocal frames from train data...\")\n",
    "no_vocal = np.where(df5[:,0] < 1)\n",
    "\n",
    "for i in reversed(no_vocal):\n",
    "    df2 = np.delete(df2, i, axis=0)\n",
    "    df5 = np.delete(df5, i, axis=0)\n",
    "\n",
    "print(\"Data's shape after remove:\", df2.shape)\n",
    "print(\"Data's shape after remove:\", df5.shape)\n",
    "\n",
    "\n",
    "# Train and test data\n",
    "X = df2\n",
    "Y = keras.utils.to_categorical(df5, num_classes=193) \n",
    "# To categorical formats the target to vectors of 0s and 1 on correspondent class\n",
    "\n",
    "\n",
    "# Create model \n",
    "print(\"Create model...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=513, activation='sigmoid'))\n",
    "model.add(Dense(500, activation='sigmoid'))\n",
    "model.add(Dense(193, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile model\n",
    "print(\"Compile model...\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# The adam optimizer was used to reduce implementation details\n",
    "\n",
    "\n",
    "# tensorboard\n",
    "tensorboard = TensorBoard(log_dir='logs/model_f0/')\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"f0.weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "model.fit(X, Y, \n",
    "              epochs=100, \n",
    "              batch_size=30, \n",
    "              validation_split=0.2, \n",
    "              callbacks=callbacks_list, \n",
    "              shuffle=False)\n",
    "print(\"> Finished\", i+1 , \"epochs\")\n",
    "# Serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"f0modelNEW.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# Serialize weights to HDF5\n",
    "model.save_weights(\"f0modelNEW.h5\")\n",
    "print(\"> Saved model to disk\")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metr