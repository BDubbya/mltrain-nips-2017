{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Programming in Pyro\n",
    "LEARN HOW TO CODE A PAPER WITH STATE OF THE ART FRAMEWORKS <br>\n",
    "NIPS 2017\n",
    "\n",
    "## Pyro Installation Instructions \n",
    "\n",
    "First install [PyTorch](http://pytorch.org).\n",
    "\n",
    "Install via pip:\n",
    "\n",
    "Python 2.7.*:\n",
    "```python\n",
    "pip install pyro-ppl\n",
    "```\n",
    "\n",
    "Python 3.5:\n",
    "\n",
    "```python\n",
    "pip3 install pyro-ppl\n",
    "```\n",
    "\n",
    "Install from source:\n",
    "\n",
    "```python\n",
    "git clone git@github.com:uber/pyro.git\n",
    "cd pyro\n",
    "pip install .\n",
    "```\n",
    "\n",
    "Other dependencies: note that in order to run this notebook you need matplotlib.\n",
    "\n",
    "This particular notebook can be found on the nips-2017 branch of the [GitHub repo](https://github.com/uber/pyro) in the examples/nips2017 directory.\n",
    "\n",
    "## First steps\n",
    "\n",
    "Let's start with some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Using cached pytorch-0.1.2.tar.gz\n",
      "Building wheels for collected packages: pytorch\n",
      "  Running setup.py bdist_wheel for pytorch ... \u001b[?25lerror\n",
      "  Complete output from command /home/nbcommon/anaconda3_501/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-el5p0_qp/pytorch/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmpbbsbh8ykpip-wheel- --python-tag cp36:\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-build-el5p0_qp/pytorch/setup.py\", line 17, in <module>\n",
      "      raise Exception(message)\n",
      "  Exception: You should install pytorch from http://pytorch.org\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for pytorch\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "Installing collected packages: pytorch\n",
      "  Running setup.py install for pytorch ... \u001b[?25l-^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mOperation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyro-ppl\n",
      "  Using cached pyro-ppl-0.1.2.tar.gz\n",
      "Requirement already satisfied: numpy>=1.7 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Requirement already satisfied: cloudpickle>=0.3.1 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Requirement already satisfied: graphviz>=0.8 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Requirement already satisfied: networkx>=2.0.0 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Collecting observations>=0.1.4 (from pyro-ppl)\n",
      "  Using cached observations-0.1.4.tar.gz\n",
      "Requirement already satisfied: torch in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from pyro-ppl)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from networkx>=2.0.0->pyro-ppl)\n",
      "Requirement already satisfied: pyyaml in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from torch->pyro-ppl)\n",
      "Building wheels for collected packages: pyro-ppl, observations\n",
      "  Running setup.py bdist_wheel for pyro-ppl ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/51/fb/ba/0ff962e95a4f61025ff5262bd9f36c983e728db3bc1bb1cbd4\n",
      "  Running setup.py bdist_wheel for observations ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/8b/b7/74/106ea929310039388cf7ddbddd40fe3d0b77af36e51405f653\n",
      "Successfully built pyro-ppl observations\n",
      "Installing collected packages: observations, pyro-ppl\n",
      "Successfully installed observations-0.1.4 pyro-ppl-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyro-ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw a sample from a unit normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9914\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mu = Variable(torch.zeros(1))   # mean zero\n",
    "sigma = Variable(torch.ones(1)) # unit variance\n",
    "x = dist.normal(mu, sigma)      # x is a sample from N(0,1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the log pdf of the sample as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.4103\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.normal.log_pdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also declare mu as a named parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mu = pyro.param(\"mu\", Variable(torch.zeros(1), requires_grad=True))\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The VAE\n",
    "\n",
    "#### The Model\n",
    "First we define our decoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "z_dim=20\n",
    "hidden_dim=100\n",
    "\n",
    "nn_decoder = nn.Sequential(\n",
    "    nn.Linear(z_dim, hidden_dim), \n",
    "    nn.Softplus(), \n",
    "    nn.Linear(hidden_dim, 784), \n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our (unconditioned) generative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import helper functions for Variables with requires_grad=False\n",
    "from pyro.util import ng_zeros, ng_ones \n",
    "\n",
    "def model(batch_size):\n",
    "    # register the decoder with Pyro (in particular all its parameters)\n",
    "    pyro.module(\"decoder\", nn_decoder)  \n",
    "    # sample the latent code z\n",
    "    z = pyro.sample(\"z\", dist.normal,   \n",
    "                    ng_zeros(batch_size, z_dim), \n",
    "                    ng_ones(batch_size, z_dim))\n",
    "    # decode z into bernoulli probabilities\n",
    "    bern_prob = nn_decoder(z)          \n",
    "    # return the mini-batch of sampled images\n",
    "    return pyro.sample(\"x\", dist.bernoulli, bern_prob) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pyro.condition` to condition `model` on data `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conditioned_model(x):\n",
    "    return pyro.condition(model, data={\"x\": x})(x.size(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The guide\n",
    "\n",
    "In order to do inference, we need to define a guide (a.k.a. an inference network). First we define the encoder network. Let's go ahead and define it explicitly instead of using `nn.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim=20, hidden_dim=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        # setup the non-linearity\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define the forward computation on the image x\n",
    "        # first compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(x))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_mu = self.fc21(hidden)\n",
    "        z_sigma = torch.exp(self.fc22(hidden))\n",
    "        return z_mu, z_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_encoder = Encoder()\n",
    "\n",
    "def vae_guide(x):\n",
    "    # register the encoder with Pyro\n",
    "    pyro.module(\"encoder\", nn_encoder)\n",
    "    # encode the mini-batch of images x\n",
    "    mu_z, sig_z = nn_encoder(x)\n",
    "    # sample and return the latent code z\n",
    "    return pyro.sample(\"z\", dist.normal, mu_z, sig_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "Now we're ready to do inference. First we setup our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyro.optim import Adam\n",
    "optimizer = Adam({\"lr\": 1.0e-3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup the `SVI` inference algorithm, which we will use to take gradient steps on the ELBO objective function. Note that `conditioned_model` and `vae_guide` both have the same call signature (namely they taken in a mini-batch of images `x`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyro.infer import SVI\n",
    "svi = SVI(conditioned_model, vae_guide, optimizer, loss=\"ELBO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup a basic training loop. First we setup the data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Compressed file ended before the end-of-stream marker was reached",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cf1ec84b8d15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m train_set = dset.MNIST(root='./mnist_data', train=True, \n\u001b[0;32m----> 7\u001b[0;31m                        transform=trans, download=True)\n\u001b[0m\u001b[1;32m      8\u001b[0m train_loader = torch.utils.data.DataLoader(dataset=train_set, \n\u001b[1;32m      9\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/torchvision-0.1.9-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mout_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                 raise EOFError(\"Compressed file ended before the \"\n\u001b[0m\u001b[1;32m    483\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "batch_size=250\n",
    "trans = transforms.ToTensor()\n",
    "train_set = dset.MNIST(root='./mnist_data', train=True, \n",
    "                       transform=trans, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do 5 epochs of training and report the ELBO averaged per data point for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-79ed0f85bcf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# do a training epoch over each mini-batch x returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# by the data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# wrap the mini-batch of images in a PyTorch Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, _ in train_loader:\n",
    "        # wrap the mini-batch of images in a PyTorch Variable\n",
    "        x = Variable(x.view(-1, 784))\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x)\n",
    "\n",
    "    # report training diagnostics\n",
    "    normalizer = len(train_loader.dataset)\n",
    "    print(\"[epoch %03d]  average training ELBO: %.4f\" % (epoch, -epoch_loss / normalizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So much for the VAE. For a more fleshed out implementation and some results please see the [tutorial](http://pyro.ai/examples/vae.html).\n",
    "\n",
    "## Recursion with random control flow\n",
    "\n",
    "Let's define the geometric distribution in terms of draws from a bernoulli distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4   6   3   0   2   0   9   0   2   0   0   5   1   1   6  \n"
     ]
    }
   ],
   "source": [
    "def geom(num_trials=0, bern_prob=0.5):\n",
    "    p = Variable(torch.Tensor([bern_prob]))\n",
    "    x = pyro.sample('x{}'.format(num_trials), dist.bernoulli, p)\n",
    "    if x.data[0] == 1:\n",
    "        return num_trials  # terminate recursion\n",
    "    else:\n",
    "        return geom(num_trials + 1, bern_prob)  # continue recursion\n",
    "\n",
    "# let's draw 15 samples \n",
    "for _ in range(15):\n",
    "    print(\"%d  \" % geom()),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the random variables in `geom` are generated dynamically and that different calls to `geom` can have different numbers of random variables. Also note that we take care to assign unique names to each dynamically generated random variable.\n",
    "\n",
    "If we crank down the bernoulli probability (so that the recursion tends to terminate after a larger number of steps) we get a geometric distribution with more of a tail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6   7   2   0   5   1   24   15   12   8   6   28   4   0   2  \n"
     ]
    }
   ],
   "source": [
    "for _ in range(15):\n",
    "    print(\"%d  \" % geom(bern_prob=0.1)),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIR\n",
    "\n",
    "#### The prior\n",
    "\n",
    "Let's build on `geom()` above to construct a recursive prior over images. First we need a prior over a single object in an image. Just like for the VAE, this prior is going to involve a decoder network. So we define that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import relu, sigmoid, grid_sample, affine_grid\n",
    "\n",
    "z_dim=50\n",
    "\n",
    "# this decodes latents z into (bernoulli pixel intensities for)\n",
    "# 20x20 sized objects\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=200):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, 20*20)\n",
    "\n",
    "    def forward(self, z_what):\n",
    "        h = relu(self.l1(z_what))\n",
    "        return sigmoid(self.l2(h))\n",
    "\n",
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the prior over a single object. Note that this prior uses (differentiable) spatial transformers to position the sampled object within the image. Most of the complexity in this code snippet is on the spatial transformer side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the prior probabilities for our random variables\n",
    "z_where_prior_mu = Variable(torch.Tensor([3, 0, 0]))\n",
    "z_where_prior_sigma = Variable(torch.Tensor([0.1, 1, 1]))\n",
    "z_what_prior_mu = ng_zeros(50)\n",
    "z_what_prior_sigma = ng_ones(50)\n",
    "\n",
    "def expand_z_where(z_where):\n",
    "    # Takes 3-dimensional vectors, and massages them into \n",
    "    # 2x3 matrices with elements like so:\n",
    "    # [s,x,y] -> [[s,0,x],\n",
    "    #             [0,s,y]]\n",
    "    n = z_where.size(0)\n",
    "    expansion_indices = Variable(torch.LongTensor([1, 0, 2, 0, 1, 3]))\n",
    "    out = torch.cat((ng_zeros([1, 1]).expand(n, 1), z_where), 1)\n",
    "    return torch.index_select(out, 1, expansion_indices).view(n, 2, 3)\n",
    "\n",
    "# takes the object generated by the decoder and places it \n",
    "# within a larger image with the desired pose\n",
    "def object_to_image(z_where, obj):\n",
    "    n = obj.size(0)\n",
    "    theta = expand_z_where(z_where)\n",
    "    grid = affine_grid(theta, torch.Size((n, 1, 50, 50)))\n",
    "    out = grid_sample(obj.view(n, 1, 20, 20), grid)\n",
    "    return out.view(n, 50, 50)\n",
    "\n",
    "def prior_step(t):\n",
    "    # Sample object pose. This is a 3-dimensional vector representing \n",
    "    # x,y position and size.\n",
    "    z_where = pyro.sample('z_where_{}'.format(t),\n",
    "                          dist.normal,\n",
    "                          z_where_prior_mu,\n",
    "                          z_where_prior_sigma,\n",
    "                          batch_size=1)\n",
    "\n",
    "    # Sample object code. This is a 50-dimensional vector.\n",
    "    z_what = pyro.sample('z_what_{}'.format(t),\n",
    "                         dist.normal,\n",
    "                         z_what_prior_mu,\n",
    "                         z_what_prior_sigma,\n",
    "                         batch_size=1)\n",
    "\n",
    "    # Map code to pixel space using the neural network.\n",
    "    y_att = decoder(z_what)\n",
    "\n",
    "    # Position/scale object within larger image.\n",
    "    y = object_to_image(z_where, y_att)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw a few samples from the object prior to see how this looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABeCAYAAAAHQJEfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOWZeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118340e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "samples = [prior_step(0)[0] for _ in range(8)]\n",
    "\n",
    "def show_images(samples):\n",
    "    plt.rcParams.update({'figure.figsize': [10, 1.6] })\n",
    "    f, axarr = plt.subplots(1, len(samples))\n",
    "\n",
    "    for i, img in enumerate(samples):\n",
    "        axarr[i].imshow(img.data.numpy(), cmap='gray')\n",
    "        axarr[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `prior_step` to define a recursive prior over images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geom_image_prior(x, step=0):\n",
    "    p = Variable(torch.Tensor([0.4]))\n",
    "    i = pyro.sample('i{}'.format(step), dist.bernoulli, p)\n",
    "    if i.data[0] == 1:\n",
    "        return x\n",
    "    else:\n",
    "        # add sampled object to canvas\n",
    "        x = x + prior_step(step)  \n",
    "        return geom_image_prior(x, step + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABeCAYAAAAHQJEfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGKNJREFUeJzt3UlvHNfVBuC3pu7q6oE9cBIHjYYVGAQSxRGQINlkkazz\nH/I7klWQHxIEAYJkk1UWXtirODGCII6RyJZlSZRIihTZU3XXPH2Lwj1i25A/W02xSft9NpYpimRV\nN1Avzz33XK0oChARERHRq9EX/QMQERERXWYMU0RERERzYJgiIiIimgPDFBEREdEcGKaIiIiI5sAw\nRURERDQHhikiIiKiOTBMEREREc2BYYqIiIhoDuZ5fjNN075V49aLotC+6ufy3rwc782X4/15Od6b\nl+O9eTnemy/H+/NF5xqmztrOzg4AoNFoIEkSaJoG27ZhWRYAIIoi+L6ParUqH1PSNAUAJEkif1ep\nVPDpp5/i2bNn53gVRERE3x4/+clPMBqNUK/XYZomgiAAALRaLUynUxRFAdM0kWUZAEDXdWiaBsuy\nEIYhkiQBAPmcf//73wu7FuVSh6lf/epXAIDvfe97CIIAYRii2WxieXkZALC/v48oiiRk2bYNAMiy\nTF6EyWSCTqcDoAxTv/71r/G73/1uMRdERET0DffjH/8Yruui3W6jKAqEYQigfAYDgO/70PUXXUjj\n8Vie01mWyeenaYqiKC5EmGLPFBEREdEcLnVlKo5jAIBhGKjVaojjGMfHx/Lx6XSKTqeDPM9RrVZh\nGIZ83LIsmKaJSqWCPM8BlClXfQ4RERGdPdd1sbGxgel0iiiKUKvVAACTyQQA0Gw2YRgGjo+PAQDd\nbhdJkiAIAqyurspzWtO0C9OWc6nDVL1eBwBYloU8z+G6Lur1upQAAaDf72NtbQ1BEEjZsFKpYDKZ\noFqtot1uYzQaAQDW1tZkjZaIiIhej6dPn8IwDLiuK8t76+vryPMcw+EQ4/EYpllGlNXVVRwdHaFW\nq+HZs2coirL/fW1tDVEULewaTrvUYUqtodZqNXzwwQdot9uoVCrwfR8AYNs2KpUKXNfF6uqqNJ27\nroswDLG1tYWTkxMJZa7rSpWKiIiIzp7qX7ZtG+12W1aT0jTFYDAAUAYlZTQaQdM0ZFmGTqcjDejH\nx8cSrBbtUocpFY48z8OdO3cAlDd9dXUVQFmVqtVqKIoCcRxD08rdjYPBAFmWYX9/H47jSMXK8zzZ\nVUBERERnz7IsDAYDWJY104BuGAYajQYcx0Ecx1KxUitLRVEgz3M0m00AmNlAtmiXOkyp5bnxeAyg\n3FbpOI6UBi3Lkn6qoijw/PlzAGVoWllZweHhIba2tmT91XVdCWhERER09gzDQL1eh+M4yLIM0+kU\nALC8vAzP8wBgZpUoCAKpSsVxPDMyQVWpFo27+YiIiIjmcKkrU8PhUP7b6/XQ7/dhGIYs1Q2HQywt\nLckyXrvdBgCEYYharYarV69iNBpJibHdbs/MtiAiInpdrl27BuDF7ENd12cqMqryUqlUZjZHqWHT\nSZJA13VZUWm1Wvjkk0/O9yJeUaPRwHA4RLVaRaPRAAA8f/4caZpiY2MDSZJI1WllZQWmaaIoCti2\nLbv+ut0uHj58uLBrOO1Shym1nVJNOLdtG1EUzQwAy7IMvu+j1WrJv2u1WjIaYTgcygujlgeJiIhe\nt1/84hcAytM6Go0GLMtClmUSInRdx2QygeM4SNNUTuvwfR+dTgfj8RhFUciOts3NTfzmN79ZzMV8\nDWEYIgxDbG9vw/d9WeYDyud6GIZYWlqSkPjgwQP0ej0YhoGiKKRP6tGjR9jc3FzINXzepU4PKs2O\nRiNJuSsrK9K05nkeJpMJtre3MZlMZGbF6uoqDg8PoWkaer2evEGLovjCsTNERESvgwpBzWYTURTB\n8zzYti070nVdR5ZliKIIhmFI43UQBAiCAI1GA+PxWFZd1K64i04VPyaTCfI8l2pcr9fDeDxGGIYz\nVbqtrS2MRiPoug5d1+G6LoDyWX5wcLCw6zjtUocptSXyxo0buHfvHr773e/C9315AdQMqX/96194\n8803ZQSCruu4fv06jo+P4bquVKauXLnC0QhERHQurly5AqDc4p+mKaIomlkhcRwHvu8jyzLUajUJ\nWaZpIo5jjMdjGIYhFZxut3v+F/EKgiCAbduyuqQKI0AZnCaTCSzLkoqVYRhSKInjWK43yzK02230\nej2MRqOFzom81GFKpXTf93H37l0MBgP0+32pLrmuizfeeAO6ruPp06fyJlUzLoqiwO3bt3Hv3j0A\nZXmRPVNERPQqfv7znwMon0lpmqJer6PRaMyM8VFb+3Vdx82bNwGUB/+qClMURVJ58X0f9Xod//3v\nfxGGoXw8yzJomoZGoyF9UwDw0Ucfnfclv7Lvf//7+MEPfoAwDOE4DoCy0uT7PqIowmg0QrVaBVBW\n3JaWlmRZU60+TSYTeJ4HXdfx+9//XmZULQKTAxEREdEcLnVlSqXxR48ewbIs6LoO27YlzeZ5jul0\nCsdx0G63pQR4cnKCRqOBtbU1aJqGra0tAJhJyERERF/H22+/DaCsmNi2jSzLYNu2NJSnaQrP89Bo\nNOB5HnZ2dgCUz7KdnR2Zu7S/vw+gXLaLogjvv/8+2u02+v0+gBe9RVmWoVqtygzF9fX1877kV9Lt\ndqFpGjY3N/H06VOpNA0GAwRBgFarJZUooNzN12g0ZCaVWpVS9zLP84VPQr/UYUqNRlhbW0Or1cLT\np0+R5zmWl5cBlAPA1BExalcfUO7mq1arCIIAH330kaxbTyYT6Z8iIiL6OtQzaXNzE3meo9/vYzqd\nSm9QGIbI8xyGYaDVakkfr23bSNMU0+kUSZLIUSoqdNi2jcFgIL1Fqp1FNW+rr3NZigGPHz/Gj370\nI2RZBsuycHh4CKAsgNy6dQvPnj1Dr9ebKYCMx2O0Wi2EYSgDuw3DQBzH6Ha7C9+Nf6nDlApHy8vL\nMAwDuq7LWXxA+cIkSSK/GaheqZOTE3nzmqYpPVatVosN6ERE9ErUlv3BYIAwDDGdTmfmQNm2DdM0\nsb+/L1Un9e88z4PruvB9XypM6s8qKKgwpWYuJUkCwzDkqLRFNmB/HbVaDQcHB/A8b+bZvLGxgdFo\nhLW1NUwmEwmVKysr+PDDD9FsNlGv1yVMdTodHB0d4fnz5wufhP6NCFOj0QgrKysIwxCapskbazwe\no9lsYnt7G+PxWFK7bdsYDoewbVvekEAZytSLSkRE9HWo5blbt27J2bCVSkV+YY/jGEmSIE1TrK+v\ny9BOVbFaWlpCp9PByckJgLLSdHBwIOfUqbAUxzFqtRryPIdt21LZOT2v6SLrdDro9XpIkkSuGQCe\nPHmCNE0RBAF835ejZXRdR71eR5IkGI/HsixYFAVGo5G0+SzSpQ5TqrT5wx/+EKPRCJubm/A8T9aV\n79y5g6WlJZycnMgWU6BM72maIs9zbG9vSzXqdLAiIqLFeuutt9BqtaQCA5T9QkDZluH7vvwCnOe5\nVIHUshqAL4wbeJ1u3LgBAPJzqW386kGvaRrq9Tps28be3p60lTSbTeR5jslkAk3TsL29DaBcRYnj\nGO12GycnJxIuwjDEZDJBpVKB53nyLFQDqy+60Wgky5eu68rr02q14Ps+lpaWUBSF9EFFUYQ4jhGG\nIdrtNnZ3dwGU97teryNN04X3THE3HxEREdEcLnVlSk17fe+997CzswNN07C0tCTTYA8ODvDkyRNc\nvXoVR0dHODo6AlAOSovjGNvb2wjDUM7ym06nskRIRESLtb29jZ2dHViWJVWcer0OTdOk+Vo9B0zT\nRBiGSNMUS0tLUrGyLEv6aF839bOo5bzV1VV4nie9UY7jwPM8VCoVmKYpz57RaCQbpdRpHkDZW+S6\nLvb29mCaplSmWq0WiqKQ55daUbksPb9//OMfsbe3hw8//HCm6ug4jlStms2mVNrUVHhV2VP37d13\n34WmaTNVrEW51GHql7/85aJ/BCIiek3SNEWj0UClUpEHbpqmyLIM4/EYtm1jY2MDQHn472QykfED\nKysrAMpfkk9P2H6dVM9SFEUyjifPc2lAHw6HWFpaQhRFsG175hf84+NjLC8vy2BPoGxJ0XUdvV4P\nBwcHMuRTjQzI81wOSAYg9+ii+OlPfwrf92GaJmzbnlnuvHv3Lt56662ZMwcdx8Hz588RxzFWV1fl\nOi3LQqVSwTvvvINmsykN6GrQ9qKDFHDJwxQREX1z5XkO13XR7/dlh9t4PMbW1pZMEf/Pf/4DoDw+\nTO2WM00TH3/8MYDyQXxeh+Gq3p8kSRBFkTSgq4d/kiRYX1+HZVkIwxBXr16Vn7FWq8E0TSwvL0uF\nSwWroiik/xcoRyb0+305T1ZVak73il0Eb7/99sxuPfVz5nmOO3fu4NatW3KwMVDu2nNdV8Y+qF6w\n/f19bGxs4L333sNwOJRZkqZpSn/aooMkwxQREV1Y9XodnU5HlrKuXr2KyWSC6X